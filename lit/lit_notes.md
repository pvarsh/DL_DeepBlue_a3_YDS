# Literature notes

1. [Hinton Lecture Slides](https://github.com/pvarsh/DL_DeepBlue_a3_YDS/blob/master/lit/hinton-lecture-slides-distributed-representations.pdf): simple description of distributed and localized neuron representations.

2. An introduction to word vectors on [kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).

3. [Text Understanding From Scratch](https://github.com/pvarsh/DL_DeepBlue_a3_YDS/blob/master/lit/Zhang-TextUnderstandingFromScratch.pdf): Convolutional nets trained on character-level data.

4. [A Convolutional Neural Network for Modelling Sentences](https://github.com/pvarsh/DL_DeepBlue_a3_YDS/blob/master/lit/KalchbrennerCNNForModellingSentences.pdf): Convolutional nets with a special pooling operation trained on concatenated word vectors.

5. [Semi-supervised recursive autoencoders for predicting sentiment distributions](https://github.com/pvarsh/DL_DeepBlue_a3_YDS/blob/master/lit/Socher-SemiSupervisedRecursiveAutoencoders.pdf): Recursive neural networks

6. [Sequence to Sequence Learning with Neural Networks](https://github.com/pvarsh/DL_DeepBlue_a3_YDS/blob/master/lit/Sutskever-SequenceToSequenceLearningWithNNs.pdf): Very general approach to supervised learning with recurrent neural networks.